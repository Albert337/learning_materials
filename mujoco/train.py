import os
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
import numpy as np

# è¿™é‡Œå‡è®¾ä½ å·²ç»å®ç°äº† InvertedPendulumEnv
from env import InvertedPendulumEnv  

# ========== é…ç½® ==========
TOTAL_TIMESTEPS = 50000     # æ€»è®­ç»ƒæ­¥æ•°
EVAL_INTERVAL   = 50        # æ¯éš”å¤šå°‘æ¬¡è¿­ä»£å¯è§†åŒ–
TIMESTEPS_PER_ITER = 1000   # æ¯æ¬¡è¿­ä»£è®­ç»ƒæ­¥æ•°
SAVE_DIR        = "./models"
os.makedirs(SAVE_DIR, exist_ok=True)

# ========== åˆ›å»ºç¯å¢ƒ ==========
def make_env():
    return InvertedPendulumEnv(render_mode=None)

env = DummyVecEnv([make_env])   # SB3 è¦æ±‚VecEnv

# ========== å®šä¹‰æ¨¡å‹ ==========
model = PPO("MlpPolicy", env, verbose=1)

best_reward = -np.inf   # è®°å½•æœ€ä½³å¥–åŠ±
n_iters = TOTAL_TIMESTEPS // TIMESTEPS_PER_ITER

for i in range(1, n_iters + 1):
    # è®­ç»ƒ
    model.learn(total_timesteps=TIMESTEPS_PER_ITER, reset_num_timesteps=False)

    # è¯„ä¼°
    test_env = InvertedPendulumEnv(render_mode=None)
    obs, _ = test_env.reset()
    done = False
    episode_reward = 0
    while not done:
        action, _ = model.predict(obs, deterministic=True)
        obs, reward, terminated, truncated, _ = test_env.step(action)
        done = terminated or truncated
        episode_reward += reward
    test_env.close()

    print(f"ğŸ” Iter {i} -- è¯„ä¼°å¥–åŠ±: {episode_reward:.2f}")

    # ä¿å­˜æœ€ä¼˜æ¨¡å‹
    if episode_reward > best_reward:
        best_reward = episode_reward
        save_path = os.path.join(SAVE_DIR, "ppo_pendulum_best.zip")
        model.save(save_path)
        print(f"âœ… æ¨¡å‹æ›´æ–°: å¥–åŠ±æå‡è‡³ {best_reward:.2f}ï¼Œå·²ä¿å­˜ {save_path}")

    # æ¯éš” 50 æ¬¡è¿­ä»£å¯è§†åŒ–ä¸€æ¬¡
    if i % EVAL_INTERVAL == 0:
        vis_env = InvertedPendulumEnv(render_mode="human")
        obs, _ = vis_env.reset()
        done = False
        print(f"ğŸ¬ å¯è§†åŒ– iteration {i}")
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, _ = vis_env.step(action)
            done = terminated or truncated
            vis_env.render()
        vis_env.close()

# ========== ç»“æŸ ==========
env.close()
print("ğŸš€ è®­ç»ƒå®Œæˆï¼ æœ€ä½³å¥–åŠ±:", best_reward)

